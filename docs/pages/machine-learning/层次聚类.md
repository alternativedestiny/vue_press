# 层次聚类

## 1. 相关性分析

1. 相关性系数：其范围（-1，1），正值表示正相关，负值表示负相关，0表示不相关
2. Pearson 皮尔逊

    > 两个变量中的任意一个不能一成不变，即标准差为0

    ```python
    from scipy.stats import pearsonr
    # pear：pearson相关系数
    # p：p值越小，表示相关系数越显著，一般p值在500个样本以上时有较高的可靠性
    pear, p = pearsonr(x, y)
    ```

3. Spearman 斯皮尔曼

    > 没有数据条件要求，使用较广

    ```python
    from scipy.stats import spearmanr
    rho, p = spearmanr(x, y)
    ```

4. Kendall 肯德尔

    > 计算对象是分类变量

    ```python
    from scipy.stats import kendalltau
    tau, p = kendalltau(x, y)
    ```

5. 参考[统计学之三大相关性系数](https://blog.csdn.net/t15600624671/article/details/77247822)

## 2. 分层（层次）聚类

1. 使用 sklearn 中的 AgglomerativeClustering，其过程为
   1. 将每一个元素定为一类
   2. 每一轮都合并指定距离最小的一类
   3. 直到所有的元素都归为同一类

2. 参数

    | 参数               | 默认值    | 简介               |
    | ------------------ | --------- | ------------------ |
    | n_clusters         | 2         | 聚类个数           |
    | affinity           | euclidean | 距离的类型         |
    | memory             | none      | 用于缓存输出的结果 |
    | connectivity       | none      | 连接矩阵           |
    | compute_full_tree  | auto      | 是否生成完整的树   |
    | linkage            | ward      | 指定距离           |
    | distance_threshold | none      | 距离阈值           |

   1. compute_full_tree：为`auto`时在满足n_clusters时训练停止，为`true`时会继续训练从而生成一颗完整的树
   2. 指定距离：
      - ward：最小化正在合并的集群的方差
      - complete：使用两个集合的每个观测距离的平均值。
      - average：使用两个集合的所有观测值之间的最大距离。
      - single：两个集合的所有观测值之间的距离的最小值。
   3. distance_threshold：链接距离阈值，超过该阈值，集群将不被合并。如果该值不为0，n_clusters 必须为0，compute_full_tree 必须为 true

3. 以相关性系数作为聚类依据，其数据格式如下

    | Pear | a    | b    | c    | d    |
    | ---- | ---- | ---- | ---- | ---- |
    | a    | 1    | 0.53 | 0.67 | 0.98 |
    | b    | 0.53 | 1    | 0.89 | 0.21 |
    | c    | 0.67 | 0.89 | 1    | 0.74 |
    | d    | 0.98 | 0.21 | 0.74 | 1    |

4. 聚类方法

    ```python
    import pandas as pd
    from sklearn.cluster import AgglomerativeClustering

    data = pd.DataFrame({'a': [1.0, 0.53, 0.67, 0.98],
                        'b': [0.53, 1.0, 0.89, 0.21],
                        'c': [0.67, 0.89, 1.0, 0.74],
                        'd': [0.98, 0.21, 0.74, 1.0]},
                        index=list('abcd'))

    # linkage：ward-单链接（最小距离），complete-全链接（最大距离），average-均链接（平均距离）
    sk = AgglomerativeClustering(2, linkage='ward')  # 分2个类
    sk.fit(data)
    index = data.index  # [a,b,c,d]
    # 分类结果
    out = sk.labels_  # [1 0 0 1]
    ```

    > 对应的结果：两个区域分别为[a, d]和[b, c]

## 3. 参考

- [Agglomerative层次聚类](https://blog.csdn.net/Haiyang_Duan/article/details/77995665)
