# LSTM å­¦ä¹ ç¬”è®°

## 1. LSTM

### 1.1. LSTMåŸç†

1. æ™®é€šRNNçš„é—®é¢˜ï¼š

    > åœ¨RNNä¸­ä¿¡æ¯åŸçš„è®°å¿†è¦è¿›è¿‡é•¿é€”è·‹æ¶‰æ‰èƒ½æŠµè¾¾æœ€åä¸€ä¸ªæ—¶é—´ç‚¹. ç„¶åæˆ‘ä»¬å¾—åˆ°è¯¯å·®, è€Œä¸”åœ¨åå‘ä¼ é€’å¾—åˆ°çš„è¯¯å·®çš„æ—¶å€™, ä»–åœ¨æ¯ä¸€æ­¥éƒ½ä¼š ä¹˜ä»¥ä¸€ä¸ªè‡ªå·±çš„å‚æ•° W.
    > å¦‚æœè¿™ä¸ª W æ˜¯ä¸€ä¸ªå°äº1 çš„æ•°, æ¯”å¦‚0.9. è¿™ä¸ª0.9 ä¸æ–­ä¹˜ä»¥è¯¯å·®, è¯¯å·®ä¼ åˆ°åˆå§‹æ—¶é—´ç‚¹ä¹Ÿä¼šæ˜¯ä¸€ä¸ªæ¥è¿‘äºé›¶çš„æ•°, æ‰€ä»¥å¯¹äºåˆå§‹æ—¶åˆ», è¯¯å·®ç›¸å½“äºå°±æ¶ˆå¤±äº†. æˆ‘ä»¬æŠŠè¿™ä¸ªé—®é¢˜å«åšæ¢¯åº¦æ¶ˆå¤±æˆ–è€…æ¢¯åº¦å¼¥æ•£ Gradient vanishing.
    > åä¹‹å¦‚æœ W æ˜¯ä¸€ä¸ªå¤§äº1 çš„æ•°, æ¯”å¦‚1.1 ä¸æ–­ç´¯ä¹˜, åˆ™åˆ°æœ€åå˜æˆäº†æ— ç©·å¤§çš„æ•°, RNNè¢«è¿™æ— ç©·å¤§çš„æ•°æ’‘æ­»äº†, è¿™ç§æƒ…å†µæˆ‘ä»¬å«åšæ¢¯åº¦çˆ†ç‚¸, Gradient exploding. è¿™å°±æ˜¯æ™®é€š RNN æ²¡æœ‰åŠæ³•å›å¿†èµ·ä¹…è¿œè®°å¿†çš„åŸå› .

2. LSTMï¼ˆlong short-term memoryï¼‰é•¿çŸ­æœŸè®°å¿†

    > LSTM å’Œæ™®é€š RNN ç›¸æ¯”, å¤šå‡ºäº†ä¸‰ä¸ªæ§åˆ¶å™¨. (è¾“å…¥æ§åˆ¶, è¾“å‡ºæ§åˆ¶, å¿˜è®°æ§åˆ¶).

    <img src='../images/lstm.jpg' width=600>

3. å‚è€ƒèµ„æ–™

   - [ä»€ä¹ˆæ˜¯LSTMğŸ”—](https://morvanzhou.github.io/tutorials/machine-learning/keras/2-4-B-LSTM/)
   - [å¿«é€Ÿç†è§£LSTMï¼Œä»æ‡µé€¼åˆ°è£…é€¼ğŸ”—](https://zhuanlan.zhihu.com/p/88892937)

### 1.2. LSTM é¢„æµ‹

1. å‰æï¼š

    > ä¸ªäººè®¤ä¸ºç¨‹åºçš„é¢„æµ‹çš„å…³é”®é¦–å…ˆåœ¨äºè®­ç»ƒé›†ä¸æµ‹è¯•é›†çš„é€‰æ‹©ï¼Œå¥½çš„è®­ç»ƒé›†ä¸æµ‹è¯•é›†åº”å½“æœ‰åˆç†çš„ã€å¯é¢„æœŸçš„è§„å¾‹ï¼Œè€Œä¸æ˜¯æ‹¿ä¸Šæ•°æ®å°±å¼€å§‹è®­ç»ƒï¼šåœ¨é¢„æµ‹æŸä¸€æ—¶æ®µçš„æ•°æ®æ—¶ï¼Œå¹¶ä¸åªæ˜¯ç”¨ä¹‹å‰çš„æ•°æ®ç›´æ¥åšè®­ç»ƒï¼Œè€Œåº”è¯¥è€ƒè™‘è¯¥æ•°æ®æ˜¯å¦æœ‰ä¸€å®šçš„åˆ†å¸ƒè§„å¾‹ã€‚
    æ¯”å¦‚ä¸€ä¸ªæ•°æ®æœ‰æ¯å¤©æœ‰ä¸€å®šçš„ç›¸ä¼¼æ€§ï¼Œé‚£ä¹ˆæ˜¯å¦åº”è¯¥å°†è¿‡å»å‡ å¤©åŒä¸€æ—¶åˆ»çš„æ•°æ®çº³å…¥è®­ç»ƒæ¨¡å‹ï¼Œä»è€Œåˆ†é…æœ‰é’ˆå¯¹æ€§çš„æ•°æ®é›†ã€‚ä¹‹åæ‰æ˜¯æ¨¡å‹è°ƒå‚ã€è®­ç»ƒç­‰ã€‚

1. ä»£ç è¯¦è§£

    ```python
    import pandas as pd
    import numpy as np
    import matplotlib.pyplot as plt
    import seaborn as sns
    from keras.models import Sequential
    from keras.layers import Dense, Activation
    from keras.layers import LSTM, Dropout
    from sklearn.preprocessing import MinMaxScaler
    from keras.callbacks import EarlyStopping
    from keras import losses

    # è¯»å–æ•°æ®
    file = 'xxx.csv'
    df1 = pd.read_csv(file, decimal=',', header=0, low_memory=False, error_bad_lines=False)
    df_all = pd.DataFrame()

    v1 = df_all['col1'][0:500000]
    time = df_all['time'][141120:141180]

    # æ•°æ®å½’ä¸€åŒ–å¤„ç†ï¼šå°†åŸå§‹æ•°æ®æ˜ å°„åˆ°0â€”1èŒƒå›´å†…
    data = v1.values
    data = data.astype('float32')
    data = np.reshape(data, (-1, 1))
    scaler = MinMaxScaler(feature_range=(0, 1))
    data = scaler.fit_transform(data)
    # æ ¹æ®æ•°æ®é‡åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†å¤§å°ï¼Œè¿™é‡Œçš„æ•°æ®æ˜¯1minä¸€ä¸ªç‚¹ï¼Œä¸€å¤©1440æ¡æ•°æ®
    train_size = 1380
    test_size = 60


    # åˆ›å»ºæ•°æ®é›†ï¼Œéå›ºå®šæ ¼å¼
    def create_data(start, size, day):  # èµ·å§‹ä½ç½®ï¼Œæ•°æ®é•¿åº¦ï¼Œå¤©æ•°
        x, y = [], []  # å‡ºå…¥ï¼Œè¾“å‡º
        for m in range(start, start + size):  # sizeæ¡è¾“å…¥
            old_data = []  # å…¶ä¸­ä¸€æ¡è¾“å…¥
            for n in range(day):
                old_data.append(data[m + n * 7 * 1440])  # ç¬¬nå‘¨ç¬¬mç‚¹çš„æ•°æ®
            x.append(old_data)
            y.append(data[m + 1440 * 7 * day])  # ç¬¬dayå‘¨ç¬¬iç‚¹æ•°æ®
        return np.array(x), np.array(y)

    # ç”¨è¿‡å»14å‘¨ç¬¬ä¸€å¤©çš„æ•°æ®å»é¢„æµ‹ç¬¬15å‘¨ç¬¬ä¸€å¤©çš„æ•°æ®
    days = 14  
    # x_trainï¼šè®­ç»ƒé›†è¾“å…¥ï¼Œy_trainï¼šè®­ç»ƒé›†è¾“å‡º
    x_train, y_train = create_data(0, train_size, days)
    # x_testï¼šæµ‹è¯•é›†è¾“å…¥ï¼Œy_testï¼šæµ‹è¯•é›†è¾“å‡º
    x_test, y_test = create_data(train_size, test_size, days)

    # x_train.shape(è®­ç»ƒæ•°æ®ç»„æ•°ï¼Œæ—¶é—´é—´éš”ï¼Œæ•°æ®ç»´æ•°(ç‰¹å¾ä¸ªæ•°))
    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))
    x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))
    print(x_train.shape)

    # æ¨¡å‹ç»“æ„
    model = Sequential()
    # å®šä¹‰ LSTM æ¨¡å‹ï¼Œç¬¬ä¸€ä¸ªéšè—å±‚å«æœ‰ 100 ä¸ªç¥ç»å…ƒ
    model.add(LSTM(100, input_shape=(x_train.shape[1], x_train.shape[2])))
    model.add(Dropout(0.25))  # æš‚æ—¶ä»ç½‘ç»œä¸­ç§»é™¤ç¥ç»ç½‘ç»œä¸­çš„å•å…ƒ
    model.add(Dense(1))  # è¾“å‡ºç»´æ•°
    model.add(Activation('relu'))  # æ¿€æ´»å‡½æ•°

    # ä½¿ç”¨å‡æ–¹å·®æŸå¤±å‡½æ•°ï¼Œä¼˜åŒ–å™¨Adamï¼Œè¯„ä¼°æ ‡å‡†
    model.compile(loss=losses.mean_squared_error,  # æŸå¤±å‡½æ•°mse
                optimizer='adam',  # ä¼˜åŒ–å™¨
                metrics=['mae', 'acc'])  # è¯„ä¼°æ ‡å‡†

    # æ¨¡å‹å°†ä¼šè¿›è¡Œ 30 ä¸ª epochs çš„è®­ç»ƒï¼Œæ¯ä¸ª batch çš„å¤§å°ä¸º 100
    history = model.fit(x_train, y_train, epochs=30, batch_size=100,
                        validation_data=(x_test, y_test),
                        callbacks=[EarlyStopping(monitor='val_loss', patience=10)],
                        verbose=1, shuffle=False)
    model.summary()

    # åšå‡ºé¢„æµ‹
    test_predict = model.predict(x_test)
    train_predict = model.predict(x_train)
    # é¢„æµ‹å€¼æ±‚é€†
    test_predict = scaler.inverse_transform(test_predict)
    train_predict = scaler.inverse_transform(train_predict)
    # çœŸå®å€¼æ±‚é€†
    y_test = scaler.inverse_transform(y_test)
    y_train = scaler.inverse_transform(y_train)

    ```

## 2. Attention æ³¨æ„åŠ›æœºåˆ¶
