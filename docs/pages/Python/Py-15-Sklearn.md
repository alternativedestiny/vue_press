# 15. Scikit-learn

## 1. ç®€ä»‹

1. Scikit-learnï¼ˆæ›¾å«åš scikits.learn è¿˜å«åš sklearnï¼‰æ˜¯ç”¨äº Python ç¼–ç¨‹è¯­è¨€çš„è‡ªç”±è½¯ä»¶æœºå™¨å­¦ä¹ åº“ã€‚å®ƒçš„ç‰¹å¾æ˜¯å…·æœ‰å„ç§åˆ†ç±»ã€å›å½’å’Œèšç±»ç®—æ³•ï¼ŒåŒ…æ‹¬æ”¯æŒå‘é‡æœºã€éšæœºæ£®æ—ã€æ¢¯åº¦æå‡ã€k-å¹³å‡èšç±»å’Œ DBSCANï¼Œå®ƒè¢«è®¾è®¡ååŒäº Python æ•°å€¼å’Œç§‘å­¦åº“ NumPy å’Œ SciPyã€‚
2. å®‰è£…

    ```bash
    # å¿…é¡»ç”¨ scikit-learnï¼Œç”¨ sklearn å®‰è£…çš„æ˜¯é”™è¯¯çš„
    pip install scikit-learn
    ```

3. å®˜æ–¹ [ä¸­æ–‡æ–‡æ¡£ğŸ”—](https://sklearn.apachecn.org/#/)
4. [èšç±»ç®—æ³•å¯¹æ¯”ğŸ”—](https://scikit-learn.org/stable/modules/clustering.html#clustering)

    ![å›¾ 1](../images/2022-10-08_9.png)  

## 2. å¸¸ç”¨å‡½æ•°

### 2.1. å½’ä¸€åŒ–

1. z-score å½’ä¸€åŒ–

    ```python
    from sklearn.preprocessing import StandardScaler
    import numpy as np

    data = np.array([1, 2, 3, 4, 5])  # [1 2 3 4 5]
    scaler = StandardScaler()  # åˆ›å»ºç¼©æ”¾å™¨
    data = np.reshape(data, (-1, 1))  # æ•°æ®å˜æˆ n*1 [[1], [2], [3], [4], [5]]
    # éœ€è¦è¾“å…¥åˆ—æ•°æ®ï¼Œ
    data = scaler.fit_transform(data)  # [[-1.41421356] [-0.70710678] [ 0.        ] [ 0.70710678] [ 1.41421356]]
    print(scaler.mean_)  # [3.]
    ```

2. min_max å½’ä¸€åŒ–

    ```python
    from sklearn.preprocessing import MinMaxScaler
    import numpy as np

    # å½’ä¸€åŒ–èŒƒå›´ 0~1
    data = np.array([1, 2, 3, 4, 5])    # [1 2 3 4 5]
    scaler = MinMaxScaler(feature_range=(0, 1)) # åˆ›å»ºç¼©æ”¾å™¨ï¼ŒèŒƒå›´ 0~1
    data = np.reshape(data, (-1, 1))  # æ•°æ®å˜æˆ n*1 [[1], [2], [3], [4], [5]]
    data = scaler.fit_transform(data)  # [[0.  ], [0.25], [0.5 ], [0.75], [1.  ]]
    ```

3. å½’ä¸€åŒ–è¿˜åŸ

    ```python
    # æ¥ç»­ä¸Šè¿°ä»£ç 
    data = scaler.inverse_transform(data)   # [[1.], [2.], [3.], [4.], [5.]]
    ```

### 2.2. åˆ†é…è®­ç»ƒé›†å’Œæµ‹è¯•é›†

1. train_test_split

    ```python
    from sklearn.model_selection import train_test_split

    x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.1, random_state=42)
    # x_data: è¦åˆ’åˆ†çš„æ ·æœ¬ç‰¹å¾é›†
    # y_data: è¦åˆ’åˆ†çš„æ ·æœ¬ç»“æœ
    # test_size: æ ·æœ¬å æ¯”ï¼Œå¦‚æœæ˜¯æ•´æ•°çš„è¯å°±æ˜¯æ ·æœ¬çš„æ•°é‡
    # random_state: éšæœºæ•°çš„ç§å­
    ```

## 3. å±‚æ¬¡èšç±» Hierarchical clustering

1. ä½¿ç”¨ sklearn ä¸­çš„ AgglomerativeClusteringï¼Œå…¶è¿‡ç¨‹ä¸º
   1. å°†æ¯ä¸€ä¸ªå…ƒç´ å®šä¸ºä¸€ç±»
   2. æ¯ä¸€è½®éƒ½åˆå¹¶æŒ‡å®šè·ç¦»æœ€å°çš„ä¸€ç±»
   3. ç›´åˆ°æ‰€æœ‰çš„å…ƒç´ éƒ½å½’ä¸ºåŒä¸€ç±»

2. èšç±»å‚æ•°

    | å‚æ•°               | é»˜è®¤å€¼    | ç®€ä»‹               |
    | ------------------ | --------- | ------------------ |
    | n_clusters         | 2         | èšç±»ä¸ªæ•°           |
    | affinity           | euclidean | è·ç¦»çš„ç±»å‹         |
    | memory             | none      | ç”¨äºç¼“å­˜è¾“å‡ºçš„ç»“æœ |
    | connectivity       | none      | è¿æ¥çŸ©é˜µ           |
    | compute_full_tree  | auto      | æ˜¯å¦ç”Ÿæˆå®Œæ•´çš„æ ‘   |
    | linkage            | ward      | æŒ‡å®šè·ç¦»           |
    | distance_threshold | none      | è·ç¦»é˜ˆå€¼           |

   1. compute_full_treeï¼šä¸º`auto`æ—¶åœ¨æ»¡è¶³ n_clusters æ—¶è®­ç»ƒåœæ­¢ï¼Œä¸º`true`æ—¶ä¼šç»§ç»­è®­ç»ƒä»è€Œç”Ÿæˆä¸€é¢—å®Œæ•´çš„æ ‘
   2. æŒ‡å®šè·ç¦»ï¼š
      - wardï¼šæœ€å°åŒ–æ­£åœ¨åˆå¹¶çš„é›†ç¾¤çš„æ–¹å·®
      - completeï¼šä½¿ç”¨ä¸¤ä¸ªé›†åˆçš„æ¯ä¸ªè§‚æµ‹è·ç¦»çš„å¹³å‡å€¼ã€‚
      - averageï¼šä½¿ç”¨ä¸¤ä¸ªé›†åˆçš„æ‰€æœ‰è§‚æµ‹å€¼ä¹‹é—´çš„æœ€å¤§è·ç¦»ã€‚
      - singleï¼šä¸¤ä¸ªé›†åˆçš„æ‰€æœ‰è§‚æµ‹å€¼ä¹‹é—´çš„è·ç¦»çš„æœ€å°å€¼ã€‚
   3. distance_thresholdï¼šé“¾æ¥è·ç¦»é˜ˆå€¼ï¼Œè¶…è¿‡è¯¥é˜ˆå€¼ï¼Œé›†ç¾¤å°†ä¸è¢«åˆå¹¶ã€‚å¦‚æœè¯¥å€¼ä¸ä¸º 0ï¼Œn_clusters å¿…é¡»ä¸º 0ï¼Œcompute_full_tree å¿…é¡»ä¸º true

    ```python
    from sklearn.cluster import AgglomerativeClustering

    zones = 10  # èšç±»ä¸ªæ•°ï¼Œå³èšæˆå‡ ç±»
    data = pd.read_csv()    # èšç±»æ•°æ®ï¼Œæ¯”å¦‚ Pearson ç›¸å…³æ€§ç³»æ•°
    
    # èšç±»å™¨ï¼ˆèšç±»ä¸ªæ•°ï¼Œèšç±»æ–¹æ³•ï¼‰
    sk = AgglomerativeClustering(zones, linkage='ward')
    # èšç±»
    sk.fit(data)

    # è¾“å‡ºç»“æœ
    out = sk.labels_    # æ¯ä¸ªæ•°æ®çš„ç±»åˆ«ç¼–å·
    ```

3. å‚æ•°

    | linkage  | ä¸­æ–‡å | è¯´æ˜     |
    | -------- | ------ | -------- |
    | ward     | å•é“¾æ¥ | æœ€å°è·ç¦» |
    | complete | å…¨é“¾æ¥ | æœ€å¤§è·ç¦» |
    | average  | å‡é“¾æ¥ | å¹³å‡è·ç¦» |

4. ç¤ºä¾‹ï¼šä»¥ç›¸å…³æ€§ç³»æ•°ä½œä¸ºèšç±»ä¾æ®ï¼Œå…¶æ•°æ®æ ¼å¼å¦‚ä¸‹

    | Pear | a    | b    | c    | d    |
    | ---- | ---- | ---- | ---- | ---- |
    | a    | 1    | 0.53 | 0.67 | 0.98 |
    | b    | 0.53 | 1    | 0.89 | 0.21 |
    | c    | 0.67 | 0.89 | 1    | 0.74 |
    | d    | 0.98 | 0.21 | 0.74 | 1    |

    ```python
    import pandas as pd
    from sklearn.cluster import AgglomerativeClustering

    data = pd.DataFrame({'a': [1.0, 0.53, 0.67, 0.98],
                        'b': [0.53, 1.0, 0.89, 0.21],
                        'c': [0.67, 0.89, 1.0, 0.74],
                        'd': [0.98, 0.21, 0.74, 1.0]},
                        index=list('abcd'))

    # linkageï¼šward-å•é“¾æ¥ï¼ˆæœ€å°è·ç¦»ï¼‰ï¼Œcomplete-å…¨é“¾æ¥ï¼ˆæœ€å¤§è·ç¦»ï¼‰ï¼Œaverage-å‡é“¾æ¥ï¼ˆå¹³å‡è·ç¦»ï¼‰
    sk = AgglomerativeClustering(2, linkage='ward')  # åˆ† 2 ä¸ªç±»
    sk.fit(data)
    index = data.index  # [a,b,c,d]
    # åˆ†ç±»ç»“æœ
    out = sk.labels_  # [1 0 0 1]
    ```

    > å¯¹åº”çš„ç»“æœï¼šä¸¤ä¸ªåŒºåŸŸåˆ†åˆ«ä¸º [a, d] å’Œ [b, c]

5. å‚è€ƒ

- [Agglomerative å±‚æ¬¡èšç±»ğŸ”—](https://blog.csdn.net/Haiyang_Duan/article/details/77995665)

## 4. k å‡å€¼èšç±» k-means

### 4.1. K-Means ç®€ä»‹

1. K-Means åŸç†ï¼šå¯¹äºç»™å®šçš„æ ·æœ¬é›†ï¼ŒæŒ‰ç…§æ ·æœ¬ä¹‹é—´çš„è·ç¦»å¤§å°ï¼Œå°†æ ·æœ¬é›†åˆ’åˆ†ä¸º K ä¸ªç°‡ã€‚è®©ç°‡å†…çš„ç‚¹å°½é‡ç´§å¯†çš„è¿åœ¨ä¸€èµ·ï¼Œè€Œè®©ç°‡é—´çš„è·ç¦»å°½é‡çš„å¤§
2. ä¼ ç»Ÿ K-Means æµç¨‹
   1. é€‰æ‹© k å€¼
   2. éšæœºé€‰æ‹© k ä¸ªåˆå§‹åŒ–è´¨å¿ƒ
   3. è®¡ç®—æ ·æœ¬åˆ°å„ä¸ªè´¨å¿ƒçš„è·ç¦»ï¼ŒæŒ‰ç…§è·ç¦»æœ€å°åˆ’åˆ†ç±»åˆ«
   4. é‡æ–°é€‰æ‹©è´¨å¿ƒï¼Œé‡å¤ç¬¬ 3 æ­¥éª¤
   5. è‹¥è´¨å¿ƒå‘é‡ä¸å˜åˆ™è¾“å‡ºåˆ†åŒºï¼Œè´Ÿè´£é‡æ–°é€‰æ‹©è´¨å¿ƒ
3. K-Means++ ç®—æ³•
   1. è´¨å¿ƒä¼˜åŒ–ï¼šç¬¬ä¸€ä¸ªè´¨å¿ƒéšæœºé€‰æ‹©ï¼Œä¸å½“å‰è´¨å¿ƒè·ç¦»è¾ƒè¿œçš„ç‚¹è¢«é€‰æ‹©èšç±»ä¸­å¿ƒçš„æ¦‚ç‡è¾ƒå¤§
4. K-Means è·ç¦»è®¡ç®—ä¼˜åŒ– elkan
   1. ç›®çš„æ˜¯å‡å°‘ä¸å¿…è¦çš„è·ç¦»è®¡ç®—
   2. åŸç†ï¼šä¸‰è§’å½¢ä¸¤è¾¹ä¹‹å’Œå¤§äºç­‰äºç¬¬ä¸‰è¾¹ï¼Œä¸¤è¾¹åªå·®å°äºç¬¬ä¸‰è¾¹
      1. å¦‚æœé¢„å…ˆçŸ¥é“ä¸¤ä¸ªè´¨å¿ƒçš„è·ç¦» D(j1, j2), è‹¥ç‚¹ x åˆ°æ»¡è¶³ 2D(x, j1) < D(x, j2), åˆ™ D(x, j1) < D(x, j2)
      2. D(x, j1) â‰¥ max{0, D(x, j1)-D(j1, j2)}
   3. æ ·æœ¬ç¨€ç–çš„æƒ…å†µä¸‹ä¸é€‚ç”¨
5. å¤§æ ·æœ¬ä¼˜åŒ– Mini Batch K-Means
   1. ç”¨æ ·æœ¬ä¸­çš„ä¸€éƒ¨åˆ†æ ·æœ¬ (batch size) æ¥åšä¼ ç»Ÿçš„ K-Means
   2. ç²¾åº¦ä¼šæœ‰æ‰€ä¸‹é™ï¼Œä¸€èˆ¬ä¼šå¤šè·‘å‡ æ¬¡ï¼Œé€‰æ‹©æœ€ä¼˜

### 4.2. æºç 

1. åŸºäº sklearn çš„ K-Means ç®—æ³•

   ```python
   from sklearn.cluster import KMeans
   import matplotlib.pyplot as plt
   from sklearn.datasets import make_blobs

   # åˆ›å»ºæ•°æ®é›†
   # x ä¸ºæ ·æœ¬ç‰¹å¾ï¼Œy ä¸ºæ ·æœ¬ç°‡ç±»åˆ«ï¼Œ1000 æ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬ä¸¤ä¸ªç‰¹å¾
   # å…± 4 ä¸ªç°‡ï¼Œç°‡ä¸­å¿ƒä¸º [1, 1], [1, -1], [-1, 1], [-1, -1], ç°‡æ–¹å·®ä¸ºåˆ†åˆ« [0.4, 0.4, 0.2, 0.2]
   x, y = make_blobs(n_samples=1000, n_features=2, centers=[[1, 1], [1, -1], [-1, 1], [-1, -1]],
                  cluster_std=[0.4, 0.4, 0.2, 0.2])

   # plt.scatter(x[:, 0], x[:, 1], marker='o')
   # plt.show()

   # n_clusters: K å€¼
   # max_iter: æœ€å¤§è¿­ä»£æ¬¡æ•°
   # n_init: ç”¨ä¸åŒåˆå§‹åŒ–ä¹‹å¿ƒè¿è¡Œç®—æ³•çš„æ¬¡æ•°ï¼Œé»˜è®¤ 10. K è¾ƒå¤§æ—¶å¯é€‚å½“å¢å¤§
   # init: åˆå§‹å€¼é€‰æ‹©æ–¹å¼ï¼Œéšæœºé€‰æ‹©'random', ä¼˜åŒ–è¿‡çš„'k-means++', ä¸€èˆ¬ç”¨é»˜è®¤çš„'k-means++'
   # algorithm: 'auto', 'full', 'elkan'ä¸‰ç§é€‰æ‹©ï¼Œä¸€èˆ¬ç”¨é»˜è®¤'auto'
   # random_state: ç”¨äºéšæœºäº§ç”Ÿä¸­å¿ƒçš„éšæœºåºåˆ—
   y_pred = KMeans(n_clusters=4, random_state=2).fit_predict(x)
   # ä¸‹é¢ä¸‰è¡Œä¸ä¸Šé¢è¿™å¥ç­‰ä»·
   # km = KMeans(n_clusters=4, random_state=2)
   # km.fit(x)
   # y_pred = km.predict(x)
   plt.scatter(x[:, 0], x[:, 1], c=y_pred)
   plt.show()

   ```

2. è¾“å‡ºç»“æœ

   ![k-means1](../images/k-means1.png)

### 4.3. å‚è€ƒ

- [K-Means èšç±»ç®—æ³•åŸç†ğŸ”—](https://www.cnblogs.com/pinard/p/6164214.html)

## 5. éšæœºæ£®æ— Random Forest

### 5.1. ç®€ä»‹

1. ç®€ä»‹

    > éšæœºæ£®æ—æ˜¯ç”±å¾ˆå¤šå†³ç­–æ ‘æ„æˆçš„ï¼Œä¸åŒå†³ç­–æ ‘ä¹‹é—´æ²¡æœ‰å…³è”ã€‚å½“æˆ‘ä»¬è¿›è¡Œåˆ†ç±»ä»»åŠ¡æ—¶ï¼Œæ–°çš„è¾“å…¥æ ·æœ¬è¿›å…¥ï¼Œå°±è®©æ£®æ—ä¸­çš„æ¯ä¸€æ£µå†³ç­–æ ‘åˆ†åˆ«è¿›è¡Œåˆ¤æ–­å’Œåˆ†ç±»ï¼Œæ¯ä¸ªå†³ç­–æ ‘ä¼šå¾—åˆ°ä¸€ä¸ªè‡ªå·±çš„åˆ†ç±»ç»“æœï¼Œå†³ç­–æ ‘çš„åˆ†ç±»ç»“æœä¸­å“ªä¸€ä¸ªåˆ†ç±»æœ€å¤šï¼Œé‚£ä¹ˆéšæœºæ£®æ—å°±ä¼šæŠŠè¿™ä¸ªç»“æœå½“åšæœ€ç»ˆçš„ç»“æœã€‚

2. ä»‹ç»å‚è€ƒ [éšæœºæ£®æ—ğŸ”—](https://easyai.tech/ai-definition/random-forest/)
3. åº”ç”¨åœºæ™¯ï¼šåˆ†ç±»ï¼Œå›å½’ï¼Œèšç±»ï¼Œå¼‚å¸¸æ£€æµ‹

## 6. SVM æ”¯æŒå‘é‡æœº

### 6.1. ç¨‹åº 1

1. ç¨‹åº 1 è¯´æ˜

   1. ç¨‹åºä¸»è¦åŠŸèƒ½ä¸ºäºŒç»´æ•°æ®åˆ†æˆä¸¤ç±»
   2. æ•°æ®ç±»å‹ï¼ˆäºŒç»´ï¼‰ï¼šx åæ ‡ï¼Œy åæ ‡ï¼Œæ ‡ç­¾
   3. è¾“å‡ºï¼šåˆ†ç±»å‡†ç¡®ç‡ï¼Œæ”¯æŒå‘é‡ä¸ªæ•°

2. ç¨‹åº 1 ä»£ç 

    ```python
    # ç”¨ sklearn svm è®­ç»ƒ rbf
    import os
    from sklearn.svm import SVC
    import matplotlib.pyplot as plt
    import numpy as np
    import csv
    from sklearn.model_selection import train_test_split, GridSearchCV
    path1 = os.path.abspath('.')

    # åŠ è½½æ•°æ®
    def load(name):
        file = np.loadtxt(open(name, 'rb'), delimiter=',')
        x = file[:, [0, 1]]
        y = file[:, 2]
        return x, y

    if __name__ == "__main__":
        # è¯»å–æ•°æ®ï¼Œé’ˆå¯¹äºŒç»´çº¿æ€§ä¸å¯åˆ†æ•°æ®
        data, label = load('./data/train.csv')
        # äº¤å‰éªŒè¯ 4:1
        x_train, x_test, y_train, y_test = train_test_split(data, label, test_size=.2, random_state=0)
        # è®¾ç½®å‚æ•°
        c = 5000  # æƒ©ç½šå‚æ•°
        g = 0.5  # æ ¸å‡½æ•°å‚æ•°
        # åˆå§‹åŒ–æ¨¡å‹å‚æ•°
        clf = SVC(cache_size=200, class_weight=None, coef0=0.0, C=c, decision_function_shape='ovr',
                degree=3, gamma=g, kernel='rbf', max_iter=-1, probability=False,
                random_state=None, shrinking=True, tol=0.001, verbose=False)
        clf.fit(x_train, y_train)
        # é¢„æµ‹ data
        predict_list = clf.predict(x_test)
        # é¢„æµ‹ç²¾åº¦
        precision = clf.score(x_test, y_test)
        print('ç¬¬', i+1, 'æ¬¡ precision is : ', precision*100, "%")
        # è·å–æ¨¡å‹è¿”å›å€¼
        n_Support_vector = clf.n_support_  # æ”¯æŒå‘é‡ä¸ªæ•°
        print("æ”¯æŒå‘é‡ä¸ªæ•°ä¸ºï¼š ", n_Support_vector)
        Support_vector_index = clf.support_  # æ”¯æŒå‘é‡ç´¢å¼•

    ```

### 6.2. ç¨‹åº 2

1. ç¨‹åº 2 åœ¨ç¨‹åº 1 çš„åŸºç¡€ä¸Šæ·»åŠ äº†ç”»å›¾åŠŸèƒ½

2. ç¨‹åº 2 ä»£ç 

    ```python
    # ç”¨ sklearn svm è®­ç»ƒï¼Œç”»å›¾
    import os
    from sklearn.svm import SVC
    import matplotlib.pyplot as plt
    import numpy as np
    import csv
    from sklearn.model_selection import train_test_split
    path1 = os.path.abspath('.')

    # åŠ è½½æ•°æ®
    def load(name):
        file = np.loadtxt(open(name, 'rb'), delimiter=',')
        x = file[:, [0, 1]]
        y = file[:, 2]
        return x, y

    # ç”¨è¿”å›çš„å‚æ•°ç»˜åˆ¶è¶…å¹³é¢
    def plot_point(data, label, Support_vector_index, clf, title):
        for i in range(np.shape(data)[0]):
            if label[i] == 1:
                plt.scatter(data[i][0], data[i][1], c='b', s=20)
            else:
                plt.scatter(data[i][0], data[i][1], c='y', s=20)
        for j in Support_vector_index:
            plt.scatter(data[j][0], data[j][1], s=100, c='', alpha=0.5, linewidth=1, edgecolor='g')
        # ç”»è¶…å¹³é¢
        x_min, x_max = data[:, 0].min() - 0.01, data[:, 0].max() + 0.01
        y_min, y_max = data[:, 1].min() - 0.01, data[:, 1].max() + 0.01
        h = 0.001
        xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
        plt.xlim(xx.min(), xx.max())
        plt.ylim(yy.min(), yy.max())
        plt.xticks(())
        plt.yticks(())
        z = clf.predict(np.c_[xx.ravel(), yy.ravel()])  # SVM çš„åˆ†å‰²è¶…å¹³é¢
        # Put the result into a color plot
        z = z.reshape(xx.shape)
        plt.contourf(xx, yy, z, cmap='hot', alpha=0.3)
        plt.title(title)
        plt.show()

    if __name__ == "__main__":
        # è¯»å–æ•°æ®ï¼Œé’ˆå¯¹äºŒç»´çº¿æ€§ä¸å¯åˆ†æ•°æ®
        data, label = load('./data/train.csv')
        # äº¤å‰éªŒè¯
        x_train, x_test, y_train, y_test = train_test_split(data, label, test_size=.2, random_state=0)
        # å‚æ•°è®¾ç½®
        c = 1000
        g = 0.1
        # åˆå§‹åŒ–æ¨¡å‹å‚æ•°
        clf = SVC(cache_size=200, class_weight=None, coef0=0.0, C=c, decision_function_shape='ovr',
                degree=3, gamma=g, kernel='rbf', max_iter=-1, probability=False,
                random_state=None, shrinking=True, tol=0.001, verbose=False)
        clf.fit(x_train, y_train)
        # é¢„æµ‹ data
        predict_list = clf.predict(x_test)
        # é¢„æµ‹ç²¾åº¦
        precision = clf.score(x_test, y_test)
        print('ç¬¬', i+1, 'æ¬¡ precision is : ', precision*100, "%")
        # è·å–æ¨¡å‹è¿”å›å€¼
        n_Support_vector = clf.n_support_  # æ”¯æŒå‘é‡ä¸ªæ•°
        print("æ”¯æŒå‘é‡ä¸ªæ•°ä¸ºï¼š ", n_Support_vector)
        Support_vector_index = clf.support_  # æ”¯æŒå‘é‡ç´¢å¼•
        title = 'C=' + str(c) + ', gamma=' + str(g)
        # ç”»å›¾
        plot_point(data, label, Support_vector_index, clf, title)

    ```

## 7. è°±èšç±» Spectral Clustering

1. ç¤ºä¾‹ç¨‹åº

    ```python
    import numpy as np
    from sklearn import datasets
    from sklearn.cluster import SpectralClustering
    import matplotlib.pyplot as plt

    X, y = datasets.make_blobs(n_samples=500, n_features=3, centers=5, cluster_std=[0.4, 0.3, 0.4, 0.3, 0.4],
                            random_state=11)
    # åˆ†ç±»æ•°=5
    y_pred = SpectralClustering(5).fit_predict(X)

    fig = plt.figure()

    ax = fig.add_subplot(projection='3d')
    ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y_pred)
    ax.set_xlabel('x')
    ax.set_ylabel('y')
    ax.set_zlabel('z')

    plt.show()
    ```
