# 14. Keras å­¦ä¹ ç¬”è®°

## 1. å®‰è£…

1. pip å®‰è£…

    ```bash
    # å®‰è£… cpu ç‰ˆçš„ tensorflow, æœ€æ–°ç‰ˆæœ¬çš„ python ä¸ä¸€å®šæ”¯æŒï¼Œä¸å»ºè®® python ç‰ˆæœ¬å¤ªæ–°
    pip install tensorflow
    # æˆ–è€…å®‰è£… gpu ç‰ˆçš„ tensorflowï¼ˆéœ€è¦ CUDA æ”¯æŒï¼‰
    pip install tensorflow-gpu

    # å®‰è£… keras
    pip install keras
    ```

2. ä½¿ç”¨ gpu è¿›è¡Œè®­ç»ƒéœ€è¦é…ç½® cuda ç¯å¢ƒï¼Œå‚è€ƒ [CUDA å®‰è£…](Python-01-ç¯å¢ƒ_Env.md##-5.-CUDA-å®‰è£…)

3. ä½¿ç”¨ gpu è¿›è¡Œè®­ç»ƒ

    ```python
    # åœ¨ä»£ç å¼€å¤´åŠ å…¥å¦‚ä¸‹è®¾ç½®

    import os
    os.environ["CUDA_VISIBLE_DEVICES"] = '0'    # ä½¿ç”¨ 0 å· gpu
    ```

4. keras ç»“æ„

    ![å›¾ 1](../images/2021-12-16_88.png)

## 2. keras å±‚

### 2.1. æ ¸å¿ƒå±‚

1. å…¨è¿æ¥å±‚ï¼šç¥ç»ç½‘ç»œä¸­æœ€å¸¸ç”¨çš„ï¼Œå®ç°å¯¹ç¥ç»ç½‘ç»œé‡Œçš„ç¥ç»å…ƒæ¿€æ´»

    ```python
    # unitsï¼šå…¨è¿æ¥å±‚è¾“å‡ºçš„ç»´åº¦ï¼Œå³ä¸‹ä¸€å±‚ç¥ç»å…ƒçš„ä¸ªæ•°
    # activationï¼šæ¿€æ´»å‡½æ•°ï¼Œé»˜è®¤ä½¿ç”¨ Relu
    # use_biasï¼šæ˜¯å¦ä½¿ç”¨ bias åç½®
    Dense(units, activation='relu', use_base=True)

    # ä½œä¸ºè¾“å…¥å±‚ï¼šè¾“å…¥ä¸º 60*1 ç»´æ•°æ®ï¼Œæ¿€æ´»å‡½æ•° relu
    model.add(Dense(200, input_shape=(60,), activation='relu'))

    # ä½œä¸ºä¸­é—´å±‚ï¼ˆéšå«å±‚ï¼‰
    model.add(Dense(200, activation='relu'))

    # ä½œä¸ºè¾“å‡ºå±‚ï¼Œunits = è¾“å‡ºç»´æ•°
    model.add(Dense(60))
    ```

   [å…¨è¿æ¥å±‚ç”¨æ³•ğŸ”—](https://blog.csdn.net/weixin_44551646/article/details/112911215)

2. æ¿€æ´»å±‚ï¼šå¯¹ä¸Šä¸€å±‚çš„

   ```python
   # æ¿€æ´»å‡½æ•°ï¼Œreluã€tanhã€sigmoid ç­‰
   Activation(activation)
   ```

   ![å›¾ 2](../images/2022-11-23_12.png)  

3. Dropout å±‚ï¼šå¯¹ä¸Šä¸€å±‚çš„ç¥ç»å…ƒéšæœºé€‰å–ä¸€å®šæ¯”ä¾‹çš„å¤±æ´»ï¼Œä¸æ›´æ–°ï¼Œä½†æ˜¯æƒé‡ä»ç„¶ä¿ç•™ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ

   ```python
   # rateï¼šå¤±æ´»æ¯”ä¾‹ï¼Œ0-1 æµ®ç‚¹æ•°
   Droupout(rate)
   ```

4. Flatten å±‚ï¼šå°†ä¸€ä¸ªç»´åº¦å¤§äºæˆ–ç­‰äº 3 çš„é«˜ç»´çŸ©é˜µï¼Œâ€œå‹æ‰â€ä¸ºä¸€ä¸ªäºŒç»´çŸ©é˜µã€‚å³ä¿ç•™ç¬¬ä¸€ä¸ªç»´åº¦ï¼ˆå¦‚ï¼šbatch çš„ä¸ªæ•°ï¼‰ï¼Œç„¶åå°†å‰©ä¸‹ç»´åº¦çš„å€¼ç›¸ä¹˜ä½œä¸ºâ€œå‹æ‰â€çŸ©é˜µçš„ç¬¬äºŒä¸ªç»´åº¦

   ```python
   Flatten()
   ```

5. Reshape å±‚ï¼šå°†è¾“å…¥çš„ç»´åº¦é‡æ„æˆç‰¹å®šçš„ shape

   ```python
   # target_shapeï¼šç›®æ ‡çŸ©é˜µçš„ç»´åº¦ï¼Œä¸åŒ…å« batch æ ·æœ¬æ•°
   Reshape(target_shape)
   ```

6. å·ç§¯å±‚ï¼šå·ç§¯æ“ä½œåˆ†ä¸ºä¸€ç»´ã€äºŒç»´ã€ä¸‰ç»´ï¼Œåˆ†åˆ«ä¸º Conv1Dã€Conv2Dã€Conv3Dã€‚ä¸€ç»´å·ç§¯ä¸»è¦åº”ç”¨äºä»¥æ—¶é—´åºåˆ—æ•°æ®æˆ–æ–‡æœ¬æ•°æ®ï¼ŒäºŒç»´å·ç§¯é€šå¸¸åº”ç”¨äºå›¾åƒæ•°æ®ã€‚ç”±äºè¿™ä¸‰ç§çš„ä½¿ç”¨å’Œå‚æ•°éƒ½åŸºæœ¬ç›¸åŒï¼Œæ‰€ä»¥ä¸»è¦ä»¥å¤„ç†å›¾åƒæ•°æ®çš„ Conv2D è¿›è¡Œè¯´æ˜

   ```python
   # filtersï¼šå·ç§¯æ ¸çš„ä¸ªæ•°
   # kernel_sizeï¼šå·ç§¯æ ¸çš„å¤§å°
   # stridesï¼šæ­¥é•¿ï¼ŒäºŒç»´ä¸­é»˜è®¤ä¸º (1,1)ï¼Œä¸€ç»´ä¸­é»˜è®¤ä¸º 1
   # Paddingï¼šè¡¥â€œ0â€ç­–ç•¥ï¼Œâ€˜validâ€™æŒ‡å·ç§¯åçš„å¤§å°ä¸åŸæ¥çš„å¤§å°å¯ä»¥ä¸åŒï¼Œâ€˜sameâ€™æŒ‡å·ç§¯åçš„å¤§å°ä¸åŸæ¥å¤§å°ä¸€è‡´
   Conv2D(filters, kernel_size, strides=(1,1), padding='valid')
   ```

7. æ± åŒ–å±‚ï¼šä¸å·ç§¯å±‚ä¸€æ ·ï¼Œæœ€å¤§ç»Ÿè®¡é‡æ± åŒ–å’Œå¹³å‡ç»Ÿè®¡é‡æ± åŒ–ä¹Ÿæœ‰ä¸‰ç§ï¼Œåˆ†åˆ«ä¸º MaxPooling1Dã€MaxPooling2Dã€MaxPooling3D å’Œ AveragePooling1Dã€AveragePooling2Dã€AveragePooling3Dï¼Œç”±äºä½¿ç”¨å’Œå‚æ•°åŸºæœ¬ç›¸åŒï¼Œæ‰€ä»¥ä¸»è¦ä»¥ MaxPooling2D è¿›è¡Œè¯´æ˜

   ```python
   # poll_sizeï¼šé•¿åº¦ä¸º 2 çš„æ•´æ•° tupleï¼Œè¡¨ç¤ºåœ¨æ¨ªå‘å’Œçºµå‘çš„ä¸‹é‡‡æ ·å› å­ï¼Œä¸€ç»´åˆ™ä¸ºçºµå‘ä¸‹é‡‡æ ·å› å­
   MaxPolling(poll_size=(2,2), strides=None, padding='valid')
   # ä¸Šé¢çš„ä»£ç è¡¨ç¤º
   ```

8. å¾ªç¯å±‚ï¼šå¾ªç¯ç¥ç»ç½‘ç»œä¸­çš„ RNNã€LSTM å’Œ GRU éƒ½ç»§æ‰¿æœ¬å±‚ï¼Œæ‰€ä»¥è¯¥çˆ¶ç±»çš„å‚æ•°åŒæ ·ä½¿ç”¨äºå¯¹åº”çš„å­ç±» SimpleRNNã€LSTM å’Œ GRU

   ```python
   # retrun_sequencesï¼šæ§åˆ¶è¿”å›ç±»å‹ï¼Œâ€˜Falseâ€™è¿”å›è¾“å‡ºåºåˆ—çš„æœ€åä¸€ä¸ªè¾“å‡ºï¼Œâ€˜Trueâ€™åˆ™è¿”å›æ•´ä¸ªåºåˆ—
   Recurrent(return_sequences=False)
   ```

9. åµŒå…¥å±‚ï¼šè¯¥å±‚åªèƒ½ç”¨åœ¨æ¨¡å‹çš„ç¬¬ä¸€å±‚ï¼Œæ˜¯å°†æ‰€æœ‰ç´¢å¼•æ ‡å·çš„ç¨€ç–çŸ©é˜µæ˜ å°„åˆ°è‡´å¯†çš„ä½ç»´çŸ©é˜µã€‚å¦‚æˆ‘ä»¬å¯¹æ–‡æœ¬æ•°æ®è¿›è¡Œå¤„ç†æ—¶ï¼Œæˆ‘ä»¬å¯¹æ¯ä¸ªè¯ç¼–å·åï¼Œæˆ‘ä»¬å¸Œæœ›å°†è¯ç¼–å·å˜æˆè¯å‘é‡å°±å¯ä»¥ä½¿ç”¨åµŒå…¥å±‚

   ```python
   # input_dimï¼šå¤§æˆ–ç­‰äº 0 çš„æ•´æ•°ï¼Œå­—å…¸é•¿åº¦ï¼Œå³è¾“å…¥æ•°æ®æœ€å¤§ä¸‹æ ‡+1
   # output_dimï¼šå¤§äº 0 çš„åˆ™è¡Œæ•°ï¼Œä»£è¡¨å…¨è¿æ¥åµŒå…¥çš„ç»´åº¦
   # input_lengthï¼šå½“è¾“å…¥åºåˆ—çš„é•¿åº¦å›ºå®šæ—¶ï¼Œè¯¥å€¼ä¸ºå…¶é•¿åº¦ã€‚å¦‚æœè¦åœ¨è¯¥å±‚åæ¥ Flatten å±‚ï¼Œç„¶åæ¥ Dense å±‚ï¼Œåˆ™å¿…é¡»æŒ‡å®šè¯¥å‚æ•°ï¼Œå¦åˆ™ Dense å±‚çš„è¾“å‡ºç»´åº¦æ— æ³•è‡ªåŠ¨æ¨æ–­
   Embedding(input_dim, output_dim, input_length)
   ```

10. [è§†é¢‘æ•™ç¨‹ğŸ”—](https://www.bilibili.com/video/BV1hE411t7RN?p=18)

### 2.2. æ±‚è§£ compile

1. åˆ›å»ºæ±‚è§£

    ```python
    model.compile(loss='mean_squared_error',  # æŸå¤±å‡½æ•°
                  optimizer='adam',  # ä¼˜åŒ–å™¨
                  metrics=['accuracy'])  # å‡†ç¡®ç‡æ ‡å‡†
    ```

2. æŸå¤±å‡½æ•° loss

    | å‡½æ•°                           | ä¸­æ–‡å                   |
    | ------------------------------ | ------------------------ |
    | mean_squared_error             | å‡æ–¹è¯¯å·®                 |
    | mean_absolute_error            | å¹³å‡ç»å¯¹è¯¯å·®             |
    | mean_absolute_percentage_error | å¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·®       |
    | mean_squared_logarithmic_error | å‡æ–¹å¯¹æ•°è¯¯å·®             |
    | squared_hinge                  | å¹³æ–¹åˆé¡µï¼ˆé“°é“¾ï¼‰         |
    | hinge                          | åˆé¡µï¼ˆé“°é“¾ï¼‰             |
    | categorical_hinge              | åˆ†ç±»åˆé¡µï¼ˆé“°é“¾ï¼‰         |
    | logcosh                        | é¢„æµ‹è¯¯å·®çš„åŒæ›²ä½™å¼¦çš„å¯¹æ•° |
    | categorical_crossentropy       | åˆ†ç±»äº¤å‰ç†µ               |

3. ä¼˜åŒ–å™¨ optimizer

    | å‡½æ•°     | å¤‡æ³¨                                                                                       |
    | -------- | ------------------------------------------------------------------------------------------ |
    | SGD      | éšæœºæ¢¯åº¦ä¸‹é™                                                                               |
    | RMSprop  | RMSProp ä¼˜åŒ–å™¨ï¼ŒMSProp ä¼˜åŒ–ç®—æ³•æ˜¯ AdaGrad ç®—æ³•çš„ä¸€ç§æ”¹è¿›ã€‚å°†æ¢¯åº¦é™¤ä»¥æœ€è¿‘å¹…åº¦çš„ç§»åŠ¨å¹³å‡å€¼ã€‚ |
    | Adagrad  | Adagrad æ˜¯ä¸€ç§å…·æœ‰ç‰¹å®šå‚æ•°å­¦ä¹ ç‡çš„ä¼˜åŒ–å™¨ï¼Œå®ƒæ ¹æ®å‚æ•°åœ¨è®­ç»ƒæœŸé—´çš„æ›´æ–°é¢‘ç‡è¿›è¡Œè‡ªé€‚åº”è°ƒæ•´ã€‚   |
    | Adadelta | Adadelta æ˜¯ Adagrad çš„ä¸€ä¸ªå…·æœ‰æ›´å¼ºé²æ£’æ€§çš„çš„æ‰©å±•ç‰ˆæœ¬                                       |
    | Adam     | Adam æœ¬è´¨ä¸Šæ˜¯ RMSProp ä¸åŠ¨é‡ momentum çš„ç»“åˆ                                               |
    | Adamax   | Adam ç®—æ³•åŸºäºæ— ç©·èŒƒæ•°ï¼ˆinfinity normï¼‰çš„å˜ç§                                               |
    | Nadam    | Nesterov ç‰ˆæœ¬ Adam ä¼˜åŒ–å™¨                                                                  |

4. è¯„ä»·å‡½æ•° metricsï¼šåŒæŸå¤±å‡½æ•°ç›¸ä¼¼ï¼Œä½†æ˜¯è¯„ä»·å‡½æ•°ä¸ä¼šç”¨äºè®­ç»ƒ

5. å‚è€ƒé“¾æ¥
    1. [compile å‚æ•°è¯¦è§£ğŸ”—](https://www.jianshu.com/p/f9c6f7c94533)

### 2.3. è®­ç»ƒ fit

1. åˆ›å»ºè®­ç»ƒ

    ```python
    # æ¨¡å‹è®­ç»ƒ
    # x_train: è®­ç»ƒé›†è¾“å…¥
    # y_train: è®­ç»ƒé›†è¾“å‡º
    # validation_split: äº¤å‰éªŒè¯
    # batch_size: æ¯ç»„è®­ç»ƒ 32 ä¸ªæ•°æ®ï¼Œ60000 æ•°æ®å°±è¦è®­ç»ƒ 60000/32=1875 ç»„
    # epochs: è®­ç»ƒ 5 å›åˆ
    # verbose: 1-è®­ç»ƒè¿‡ç¨‹å¯è§†ï¼›0-è®­ç»ƒç»“æœä¸å¯è§†
    model.fit(x_train, y_train, validation_split=0.5, batch_size=100, epochs=100, verbose=1)
    # å¦‚æœæœ‰ä¸“é—¨çš„éªŒè¯é›†
    # x_test: éªŒè¯é›†è¾“å…¥
    # y_test: éªŒè¯é›†è¾“å‡º
    model.fit(x_train, y_train, validation_data=[x_test, y_test], batch_size=100, epochs=100, verbose=1)
    ```

2. å‚æ•°è®¾ç½®

    | å‚æ•°             | é…ç½®        | å¤‡æ³¨                                                               |
    | ---------------- | ----------- | ------------------------------------------------------------------ |
    | validation_split | 0~1         | é»˜è®¤ä¸è¿›è¡Œäº¤å‰éªŒè¯ï¼Œäº¤å‰éªŒè¯æ¯”ä¾‹ï¼š0.1 è¡¨ç¤ºæ¯æ¬¡ä¿ç•™ 10%æ•°æ®è¿›è¡ŒéªŒè¯ |
    | batch_size       | æ•´æ•°        | è¡¨ç¤ºæ¯æ¬¡è¿›è¡Œè®­ç»ƒçš„æ•°æ®ä¸ªæ•°ï¼Œæ•°å­—è¶Šå¤§å†…å­˜ï¼ˆæˆ–æ˜¾å­˜ï¼‰å ç”¨è¶Šå¤§         |
    | epochs           | æ•´æ•°        | æœ€å¤§è®­ç»ƒå›åˆæ•°ï¼Œå¦‚æœæ²¡æœ‰è®¾ç½®æå‰æˆªè‡³æ¡ä»¶åˆ™å°±æ˜¯è®­ç»ƒå›åˆæ•°           |
    | verbose          | 0/1ï¼Œé»˜è®¤ 0 | 1-è®­ç»ƒè¿‡ç¨‹å¯è§†ï¼›0-è®­ç»ƒç»“æœä¸å¯è§†                                   |

3. å‚æ•°å…³ç³»

    $$ æ¯ä¸ª epoch è®­ç»ƒçš„ç»„æ•° = \frac{x\_train.size() * (1-validation\_split)}{batch\_size} $$

4. æ—©åœæ³•

    ```python
    from keras import callbacks

    # monitor: loss/accuracy/val_loss/val_accuracy
    # patience: 
    callback = callbacks.EarlyStopping(monitor='loss', patience=20)
    model.fit(x_train, y_train, validation_split=0.05, batch_size=500, epochs=300, verbose=1, callbacks=[callback])
    ```

### 2.4. æ¨¡å‹ä¿å­˜ä¸åŠ è½½

1. ä¿å­˜æ¨¡å‹

    ```python
    model.save('./model/model_name.h5')
    ```

2. åŠ è½½æ¨¡å‹

    ```python
    from keras.models import load_model

    model = load_model('./model/model_name.h5')

    # è¿˜å¯ä»¥ç»§ç»­è®­ç»ƒï¼Œå‚æ•°ä¸è®­ç»ƒæ—¶çš„å‚æ•°ç›¸åŒ
    model.fit(x_train, y_train, ...)
    ```

3. æ¨¡å‹ä½¿ç”¨

    ```python
    # æ¨¡å‹è¾“å‡º
    y_pre = model.predict(x_input)
    ```

## 3. æ¨¡å‹æ­å»º

### 3.1. å¤šå±‚æ„ŸçŸ¥æœºï¼ˆå…¨é“¾æ¥/DNN) MLP

1. æ¨¡å‹æ­å»º

    ```python
    def build_model(self):  # åˆ›å»ºæ¨¡å‹
        units = 500  # ç¥ç»å…ƒ
        model = Sequential()
        # è¾“å…¥å±‚
        model.add(Dense(1000, input_shape=(60,), activation='relu'))
        # éšå«å±‚
        model.add(Dense(4 * units, activation='relu'))
        model.add(Dropout(0.2))
        model.add(Dense(2 * units, activation='relu'))
        model.add(Dropout(0.2))
        model.add(Dense(units, activation='tanh'))
        model.add(Dropout(0.1))
        # è¾“å‡ºå±‚
        model.add(Dense(60))

        # æ˜¾ç¤ºæ¨¡å‹
        model.summary()

        self.model = model  # åœ¨ç±»ä¸­æ—¶å°†æ¨¡å‹èµ‹ç»™ç±»å˜é‡
    ```

2. æ¨¡å‹è®­ç»ƒ

    ```python
    def train(self):  # æ¨¡å‹è®­ç»ƒ
        model = self.model

        model.compile(loss='mse',  # æŸå¤±å‡½æ•°
                        optimizer='adam',  # ä¼˜åŒ–å™¨
                        metrics=['accuracy', 'mae'])  # å‡†ç¡®ç‡æ ‡å‡†

        x_train, y_train = self.x_data, self.y_data

        # è®­ç»ƒæ¨¡å‹
        callback = callbacks.EarlyStopping(monitor='accuracy', patience=5)
        model.fit(x_train, y_train, validation_split=0.05, batch_size=500, epochs=300, verbose=1, callbacks=[callback])

        # ä¿å­˜è®­ç»ƒè¿‡ç¨‹
        df_his = pd.DataFrame({
            'loss': model.history.history['loss'],
            'acc': model.history.history['accuracy'],
            'val_loss': model.history.history['val_loss'],
            'val_acc': model.history.history['val_accuracy']
        })
        df_his.to_csv('../data/model_his.csv', index=False)

        # ä¿å­˜æ¨¡å‹
        model.save(self.model_name)
        print('ä¿å­˜æ¨¡å‹ï¼š', self.model_name)
    ```

### 3.2. æ‰‹å†™æ•°å­—è¯†åˆ«ï¼Œ[åŸæ–‡é“¾æ¥ğŸ”—](https://cloud.tencent.com/developer/article/1829972)

1. åŠ è½½æ•°æ®ï¼Œç¬¬ä¸€æ¬¡è¿è¡Œä¼šè”ç½‘ä¸‹è½½æ•°æ®ï¼ˆå¯å¤åˆ¶è¿è¡Œï¼‰

    ```python
    from keras.datasets import mnist
    import matplotlib.pyplot as plt

    # ï¼ˆè®­ç»ƒé›†è¾“å…¥ï¼Œè®­ç»ƒé›†è¾“å‡ºï¼‰, ï¼ˆæµ‹è¯•é›†è¾“å…¥ï¼Œæµ‹è¯•é›†è¾“å‡ºï¼‰ = ä» mnist ä¸­åŠ è½½æ•°æ®é›†
    (trainX, trainY), (testX, testY) = mnist.load_data()
    # è®­ç»ƒé›† 60000 ä¸ª 28*28 åƒç´ ä¸ªæ‰‹å†™æ•°å­—å›¾ç‰‡
    print('Train: X=%s, Y=%s' % (trainX.shape, trainY.shape))   # Train: X=(60000, 28, 28), Y=(60000,)
    # æµ‹è¯•é›† 10000 ä¸ª 28*28 åƒç´ ä¸ªæ‰‹å†™æ•°å­—å›¾ç‰‡
    print('Test: X=%s, Y=%s' % (testX.shape, testY.shape))      # Test: X=(10000, 28, 28), Y=(10000,)
    # ç”»å‡ºä¸€äº›å›¾
    for i in range(9):
        plt.subplot(330 + 1 + i)
        plt.imshow(trainX[i])
    plt.show()
    ```

    ![å›¾ 1](../images/2022-11-21_45.png)  

2. ç¥ç»ç½‘ç»œæ¨¡å‹åˆ›å»º & è®­ç»ƒï¼ˆå¯å¤åˆ¶è¿è¡Œï¼‰

    ```python
    from keras.datasets import mnist
    from keras import Sequential
    from keras.layers import Dense, Dropout, Activation, Flatten  # å¸¸ç”¨å±‚
    from keras.layers import Conv2D, MaxPool2D  # å·ç§¯å±‚ï¼Œæ± åŒ–å±‚
    from keras.utils import np_utils

    # ï¼ˆè®­ç»ƒé›†è¾“å…¥ï¼Œè®­ç»ƒé›†è¾“å‡ºï¼‰, ï¼ˆæµ‹è¯•é›†è¾“å…¥ï¼Œæµ‹è¯•é›†è¾“å‡ºï¼‰ = ä» mnist ä¸­åŠ è½½æ•°æ®é›†
    (trainX, trainY), (testX, testY) = mnist.load_data()

    # æ•°æ®é›†ä»å½¢çŠ¶ï¼ˆnï¼Œå®½åº¦ï¼Œé«˜åº¦ï¼‰è½¬æ¢ä¸ºï¼ˆnï¼Œå®½åº¦ï¼Œé«˜åº¦ï¼Œæ·±åº¦ï¼‰
    X_train = trainX.reshape(trainX.shape[0], 28, 28, 1)
    X_test = testX.reshape(testX.shape[0], 28, 28, 1)
    print(X_train.shape)  # (60000, 28, 28, 1)

    # å°† 1 ç»´ç±»æ•°ç»„è½¬æ¢ä¸º 10 ç»´ç±»çŸ©é˜µ
    Y_train = np_utils.to_categorical(trainY, 10)
    Y_test = np_utils.to_categorical(testY, 10)
    print(Y_train.shape)  # (60000, 10), é™¤äº†å¯¹åº”ä½æ˜¯ 1 ä»¥å¤–éƒ½æ˜¯ 0

    # æ¨¡å‹ç»“æ„
    model = Sequential()  # åˆ›å»ºä¸€ä¸ªåºåˆ—
    # å·ç§¯å±‚ï¼š32 ä¸ªå·ç§¯æ ¸ï¼Œå·ç§¯æ ¸ 3*3, æ¿€æ´»å‡½æ•° relu, æ¨¡å‹è¾“å…¥ (28, 28, 1)
    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))
    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu'))  # ç¬¬äºŒå±‚å·ç§¯å±‚
    model.add(MaxPool2D(pool_size=(2, 2)))  # æ± åŒ–å±‚
    model.add(Dropout(0.25))  # Dropout å±‚
    model.add(Flatten())  # Flatten å±‚ï¼šå°†é«˜ä½çŸ©é˜µå‹ç¼©æˆ 2 ç»´çŸ©é˜µ
    model.add(Dense(128, activation='relu'))  # ä¸­é—´å±‚ï¼šå…¨é“¾æ¥ï¼Œè¾“å‡ºç»´åº¦ (128)
    model.add(Dropout(0.5))  # Dropout å±‚
    model.add(Dense(10, activation='softmax'))  # è¾“å‡ºå±‚ï¼šå…¨é“¾æ¥ï¼Œè¾“å‡ºç»´åº¦ (10)

    model.summary()  # æ‰“å°æ¨¡å‹ä¿¡æ¯

    model.compile(loss='categorical_crossentropy',
                  optimizer='adam',
                  metrics=['accuracy'])

    # æ¨¡å‹è®­ç»ƒ
    # X_train: è®­ç»ƒé›†è¾“å…¥
    # Y_train: è®­ç»ƒé›†è¾“å‡º
    # batch_size: æ¯ç»„è®­ç»ƒ 32 ä¸ªæ•°æ®ï¼Œ60000 æ•°æ®å°±è¦è®­ç»ƒ 60000/32=1875 ç»„
    # epochs: è®­ç»ƒ 5 å›åˆ
    # verbose: 1-è®­ç»ƒè¿‡ç¨‹å¯è§†ï¼›0-è®­ç»ƒç»“æœä¸å¯è§†
    model.fit(X_train, Y_train, batch_size=32, epochs=5, verbose=1)

    # æ¨¡å‹è¯„ä¼°
    # X_test: æµ‹è¯•é›†è¾“å…¥
    # Y_test: æµ‹è¯•é›†è¾“å‡º
    # batch_size: é»˜è®¤ 32
    score = model.evaluate(X_test, Y_test, verbose=0)
    print(score)
    ```

3. æ¨¡å‹ä¿¡æ¯ï¼Œmodel.summary() æ‰“å°ç»“æœ

    ```bash
    Model: "sequential"
    _________________________________________________________________
    # å±‚                        è¾“å‡ºç»´åº¦                   å‚æ•°ä¸ªæ•°
    Layer (type)                 Output Shape              Param #    
    =================================================================
    conv2d (Conv2D)              (None, 26, 26, 32)        320       
    _________________________________________________________________
    conv2d_1 (Conv2D)            (None, 24, 24, 32)        9248      
    _________________________________________________________________
    max_pooling2d (MaxPooling2D) (None, 12, 12, 32)        0         
    _________________________________________________________________
    dropout (Dropout)            (None, 12, 12, 32)        0         
    _________________________________________________________________
    flatten (Flatten)            (None, 4608)              0         
    _________________________________________________________________
    dense (Dense)                (None, 128)               589952    
    _________________________________________________________________
    dropout_1 (Dropout)          (None, 128)               0         
    _________________________________________________________________
    dense_1 (Dense)              (None, 10)                1290      
    =================================================================

    Total params: 600,810       # æ€»å‚æ•°ä¸ªæ•°
    Trainable params: 600,810   # å¯è®­ç»ƒå‚æ•°
    Non-trainable params: 0     # ä¸å¯è®­ç»ƒå‚æ•°
    _________________________________________________________________
    ```

4. æ¨¡å‹è®­ç»ƒä¿¡æ¯

    ```bash
    Epoch 1/5   # ç¬¬ 1 å›åˆ
    # è®­ç»ƒé›† 60000, 32 ä¸ªæ•°æ®ä¸ºä¸€ç»„è®­ç»ƒï¼Œè®­ç»ƒ 60000/32=1875 ç»„ï¼›å›åˆè€—æ—¶ï¼Œæ¯ç»„è€—æ—¶ï¼›æŸå¤±ï¼›ç²¾åº¦
    1875/1875 [==============================] - 55s 29ms/step - loss: 0.4725 - accuracy: 0.8922
    Epoch 2/5   # ç¬¬ 2 å›åˆ
    1875/1875 [==============================] - 64s 34ms/step - loss: 0.1421 - accuracy: 0.9591
    Epoch 3/5   # ç¬¬ 3 å›åˆ
    1875/1875 [==============================] - 62s 33ms/step - loss: 0.1103 - accuracy: 0.9685
    Epoch 4/5   # ç¬¬ 4 å›åˆ
    1875/1875 [==============================] - 65s 35ms/step - loss: 0.0964 - accuracy: 0.9719
    Epoch 5/5   # ç¬¬ 5 å›åˆ
    1875/1875 [==============================] - 61s 33ms/step - loss: 0.0841 - accuracy: 0.9749
    ```

### 3.3. é•¿çŸ­æœŸè®°å¿† LSTM

1. å•å±‚ lstm é¢„æµ‹æ¨¡å‹

    ```python
    # x_train è®­ç»ƒé›†è¾“å…¥
    # x_train.shape[0] è¡¨ç¤º x_train æœ‰å¤šå°‘æ¡
    # x_train.shape[1] è¡¨ç¤º x_train æ¯æ¡æ•°æ®æœ‰å‡ ä¸ªå€¼
    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))
    # x_test æµ‹è¯•é›†è¾“å…¥
    x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))
    print(x_train.shape)
    
    # æ¨¡å‹ç»“æ„
    model = Sequential()
    # å®šä¹‰ LSTM æ¨¡å‹ï¼Œç¬¬ä¸€ä¸ªéšè—å±‚å«æœ‰ 100 ä¸ªç¥ç»å…ƒ
    model.add(LSTM(100, input_shape=(x_train.shape[1], x_train.shape[2])))
    model.add(Dropout(0.25))  # æš‚æ—¶ä»ç½‘ç»œä¸­ç§»é™¤ç¥ç»ç½‘ç»œä¸­çš„å•å…ƒ
    model.add(Dense(1))  # è¾“å‡ºç»´æ•°
    model.add(Activation('relu'))   # æ¿€æ´»å‡½æ•°

    # ä½¿ç”¨å‡æ–¹å·®æŸå¤±å‡½æ•°ï¼Œä¼˜åŒ–å™¨ Adamï¼Œè¯„ä¼°æ ‡å‡†
    model.compile(loss=losses.mean_squared_error,  # æŸå¤±å‡½æ•°
                optimizer='adam',  # ä¼˜åŒ–å™¨
                metrics=['mae'])  # è¯„ä¼°æ ‡å‡†

    # æ¨¡å‹å°†ä¼šè¿›è¡Œ 30 ä¸ª epochsï¼ˆå›åˆï¼‰çš„è®­ç»ƒï¼Œæ¯ä¸ªå›åˆå°†æ•°æ®åˆ†æˆ batch=100 çš„ç»„è¿›è¡Œè®­ç»ƒ
    # æ¯”å¦‚æœ‰ 1000 æ¡è®­ç»ƒæ•°æ®ï¼Œbatch_size=100 è¡¨ç¤ºå°† 1000 æ¡æ•°æ®åˆ†æˆ 10 ç»„ï¼Œæ¯ç»„ 100 æ¡æ•°æ®ï¼Œé‡å¤è¿›è¡Œ epochs æ¬¡è®­ç»ƒ
    history = model.fit(x_train, y_train, epochs=30, batch_size=100,
                        validation_data=(x_test, y_test),   # éªŒè¯é›†
                        callbacks=[EarlyStopping(monitor='val_loss', patience=10)], # å½“è¢«æ£€æµ‹å€¼ä¸å†æå‡ï¼Œæå‰ç»“æŸè®­ç»ƒ
                        verbose=1,  # æ—¥å¿—æ˜¾ç¤ºï¼ˆé»˜è®¤ 1): 0-ä¸åœ¨æ ‡å‡†è¾“å‡ºæµè¾“å‡ºæ—¥å¿—ä¿¡æ¯ï¼›1-è¾“å‡ºè¿›åº¦æ¡è®°å½•ï¼›2-æ¯ä¸ª epoch è¾“å‡ºä¸€è¡Œè®°å½•
                        shuffle=True)   # shuffle=False ä¸æ‰“ä¹±æ•°æ®é¡ºåºï¼Œä¸€èˆ¬è®¾ç½®ä¸º True è®­ç»ƒç»“æœä¼šå¥½ä¸€äº›
    model.summary() # æ‰“å°æ¨¡å‹ä¿¡æ¯

    # åšå‡ºé¢„æµ‹
    test_predict = model.predict(x_test)
    # é¢„æµ‹å€¼æ±‚é€†
    test_predict = scaler.inverse_transform(test_predict)
    # çœŸå®å€¼æ±‚é€†
    y_test = scaler.inverse_transform(y_test)
    ```

2. å¤šå±‚ lstm é¢„æµ‹æ¨¡å‹

    ```python
    # åˆ›å»ºç¥ç»ç½‘ç»œ
    def build_model(x_train, y_train):
        # æ¨¡å‹ç»“æ„
        model = Sequential()
        # å®šä¹‰ LSTM æ¨¡å‹
        model.add(LSTM(100, input_shape=(x_train.shape[1], x_train.shape[2]), return_sequences=True))  # ç¬¬ä¸€å±‚ lstm
        model.add(LSTM(60, return_sequences=False))  # ç¬¬äºŒå±‚ lstm
        model.add(Dropout(0.2))  # æš‚æ—¶ä»ç½‘ç»œä¸­ç§»é™¤ç¥ç»ç½‘ç»œä¸­çš„å•å…ƒ
        model.add(Dense(1))  # è¾“å‡ºç»´æ•°
        model.add(Activation('relu'))

        # ä½¿ç”¨å‡æ–¹å·®æŸå¤±å‡½æ•°ï¼Œä¼˜åŒ–å™¨ Adamï¼Œè¯„ä¼°æ ‡å‡†
        model.compile(loss=losses.mean_squared_error,  # æŸå¤±å‡½æ•°
                    optimizer='adam',  # ä¼˜åŒ–å™¨
                    metrics=['mae'])  # è¯„ä¼°æ ‡å‡†

        model.summary()

        return model
    ```

3. attention-lstm

    ```bash
    # å®‰è£… attention åŒ…
    pip install attention
    ```

    ```python
    # åˆ›å»ºç¥ç»ç½‘ç»œ
    def build_model(x_train):
        # æ¨¡å‹ç»“æ„
        model = Sequential()
        # å®šä¹‰ LSTM æ¨¡å‹
        model.add(LSTM(100, input_shape=(x_train.shape[1], x_train.shape[2]), return_sequences=True))  # ç¬¬ä¸€å±‚ lstm
        model.add(LSTM(100, return_sequences=True))  # ç¬¬äºŒå±‚ lstm
        model.add(Attention())  # attention å±‚
        model.add(Dropout(0.1))  # æš‚æ—¶ä»ç½‘ç»œä¸­ç§»é™¤ç¥ç»ç½‘ç»œä¸­çš„å•å…ƒ
        model.add(Dense(1))  # è¾“å‡ºç»´æ•°
        model.add(Activation('relu'))  # æ¿€æ´»å‡½æ•°

        # ä½¿ç”¨å‡æ–¹å·®æŸå¤±å‡½æ•°ï¼Œä¼˜åŒ–å™¨ Adamï¼Œè¯„ä¼°æ ‡å‡†
        model.compile(loss=losses.mean_squared_error,  # æŸå¤±å‡½æ•°
                    optimizer='adam',  # ä¼˜åŒ–å™¨
                    metrics=['mae'])  # è¯„ä¼°æ ‡å‡†

        model.summary()  # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯

        return model
    ```

4. è®­ç»ƒç»“æœ

    | history  | å¤‡æ³¨                             |
    | -------- | -------------------------------- |
    | loss     | è®­ç»ƒé›†æŸå¤±ï¼ˆæ ¹æ®è®¾ç½®çš„æŸå¤±å‡½æ•°ï¼‰ |
    | mae      | è®­ç»ƒé›†å¹³å‡ç»å¯¹è¯¯å·®               |
    | acc      | è®­ç»ƒé›†å‡†ç¡®ç‡ï¼ˆç”¨äºåˆ†ç±»é—®é¢˜ï¼‰     |
    | val_loss | éªŒè¯é›†æŸå¤±ï¼ˆæ ¹æ®è®¾ç½®çš„æŸå¤±å‡½æ•°ï¼‰ |
    | val_mae  | éªŒè¯é›†å¹³å‡ç»å¯¹è¯¯å·®               |
    | val_acc  | éªŒè¯é›†å‡†ç¡®ç‡ï¼ˆç”¨äºåˆ†ç±»é—®é¢˜ï¼‰     |

    - [keras ä¸­ LSTM èƒ½ç”¨ accuracy è¿›è¡Œè¯„ä»·ä¹ˆï¼ŸğŸ”—](https://www.zhihu.com/question/432212136)

## 4. å¤‡æ³¨

- [Keras å…¥é—¨ğŸ”—](http://www.tensorflownews.com/2018/03/15/%e4%bd%bf%e7%94%a8keras%e8%bf%9b%e8%a1%8c%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%ef%bc%9a%ef%bc%88%e4%b8%80%ef%bc%89keras-%e5%85%a5%e9%97%a8/)
- [Keras ä¸­æ–‡æ–‡æ¡£ğŸ”—](https://keras.io/zh/)
